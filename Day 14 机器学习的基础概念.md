## 机器学习
### 1. 什么是机器学习？
它是实现人工智能的一种**核心方法**。简单来说，**机器学习就是让机器从数据中自动学习模式和规律，并利用这些规律对未知数据进行预测或决策，而不需要被明确地编程来执行特定任务** 

### 2. 机器学习的通俗版解释？
机器学习输入的是**数据**和**答案**，输出的是**规则** 

## 深度学习
### 3. 什么是深度学习？
它是机器学习的一个**子集**，深度学习的“深度”指的是其模型——**深度神经网络**中包含了**多个隐藏层** 

### 4. 浅层学习和深层学习的辨析？
- **浅层学习**：可能只有1个隐藏层
- **深度学习**：通常有数十个、上百个甚至更多的隐藏层。这些多层结构让模型能够从原始数据中**逐层提取越来越抽象的特征** 

## 神经网络
### 5. 什么是神经网络？
**神经网络 = 一群神经元手拉手，层层协作，共同完成复杂任务的智能系统** 

### 6. 什么是神经元？
**神经元 = 加权求和 + 激活函数** 
与生物中的神经元概念类似，都是神经网络的最小单位

> ==**打个比方**== 
> **一个公司的决策会议：** 
> **输入（xixi​）** = 各部门代表的发言（市场部、技术部、财务部...）
> **权重（wiwi​）** = CEO 对每个部门意见的**重视程度**  
>     （市场部权重高，技术部权重中等，财务部权重最高）
> **偏置（bb）** = CEO 自己的**经验直觉**（固有倾向）
> 
> **神经元的工作过程**：
> 1. CEO 综合各部门意见，并按其重视程度加权（加权求和）
> 2. 再结合自己的经验判断（偏置），形成一个初步决策倾向（zz）
> 3. 最后通过一个“拍板机制”（激活函数）——如果倾向足够强，就**正式宣布决策**（输出 aa）；否则暂缓决定（输出接近0）

### 7. 神经网络的层次架构的具体体现？

- **神经网络架构：** 
    - **输入层**：接收初始信号
    - **隐藏层**：一层或多层，对信号进行处理和变换
    - **输出层**：产生最终结果
        层与层之间的神经元通过**连接**相连，每个连接都有一个**权重**

- 输入层的神经元数量应该是按特征数量来确定的

- **一个基础的神经网络（以三层网络为例）的构成：** 
	1. **输入层**：接收原始数据的地方。每个“神经元”代表输入数据的一个特征
	   比如，对于一张32x32像素的灰度图片，输入层可能有1024个神经元，每个神经元接收一个像素的亮度值
	
	2. **隐藏层**：位于输入层和输出层之间，负责对输入数据进行一系列的非线性变换和特征提取。我们“看不到”它们直接处理输入或输出最终结果，所以叫“隐藏”。在基础网络中，可以有一个或多个隐藏层
	
	3. **输出层**：负责输出网络的最终结果。<mark style="background:rgba(240, 107, 5, 0.2)">它的形式和激活函数由任务决定</mark>
		- **二分类问题**（是/不是猫）：通常只有一个神经元，输出一个0到1之间的概率值
		- **多分类问题**（识别猫/狗/鸟）：有多个神经元，每个对应一个类别，输出层通常使用<mark style="background:rgba(240, 107, 5, 0.2)">Softmax函数</mark>，将这些输出转换为所有类别上的概率分布，总和为1

> ==**再打个比方**==
> **一家叫“神经网络”的公司**
>**公司架构 = 神经网络的层级结构**
> **想象一家处理“手写数字识别”任务的图像识别公司：**
> 
> 🟢 **输入层** = 前台接待员
> 1. 客户拿来一张手写数字“7”的图片
> 2. 前台的工作很简单：把图片拆成无数个小格子（像素），每个格子的亮度值记录下来
> 3. **不做任何思考**，只是把原始数据传递给下一层
> 4. 对应：输入层的每个神经元接收一个特征（如一个像素值）
>
> 🔵 **隐藏层** = 各部门的普通员工（一层又一层）
> **第一隐藏层** = 初级分析员（边缘检测部）
> 1. 专门看图片里有没有“横线”、“竖线”、“斜线”、“弧线”
> 2. 每个初级分析员只负责一种线条：比如A员工只报告“我看到横线了！”，B员工只报告“我看到竖线了！”
> 3. 对应：第一层神经元学习检测低级特征
> 
> **第二隐藏层** = 中级分析员（部件组合部）
> 1. 他们不看原始像素了，而是看初级分析员的报告
> 2. 负责把线条组合成简单部件：比如C员工说“我看到一个‘横线+竖线’的组合，这像是一个拐角”；D员工说“我看到两个弧线，这可能是一个圆圈的一部分”
> 3. 对应：第二层神经元组合低级特征，形成中级特征
>     
> 
> **第三隐藏层** = 高级分析员（整体识别部）
> 1. 他们看中级分析员的报告，组合出完整概念
> 2. E员工说“我看到了一个‘竖线+两个弧线’的组合，这很像数字‘8’的上面部分”；F员工说“我看到了‘横线+竖线’的组合，这很像数字‘7’的结构”
> 3. 对应：更高层神经元识别更抽象、更完整的模式
> 
> 🟠 **输出层** = 最终决策委员会
> 1. 有10位委员，分别对应数字0-9
> 2. 他们听取所有高级分析员的报告，结合自己的判断，最终投票
> 3. 对应数字“7”的委员得到的支持最多，他大声宣布：“我以95%的信心认为，这是数字7！”
> 4. 对应：输出层给出最终预测结果


## 数据集
### 8. 什么是训练集、验证集和测试集？
- **训练集：** 用于**训练**模型。模型通过反复“看”训练集中的数据和对应的正确答案，来学习其中的模式和规律，并不断调整自身的参数（权重和偏置），以减小预测值与真实值之间的差距

- **验证集：** 用于**验证**模型在训练过程中的表现，并帮助**调整模型的超参数**（如学习率、网络层数、神经元数量等）和**进行模型选择**。验证集不参与模型的参数更新，但模型的开发者会用它来评估不同设置下的模型效果，看看哪个更好。这是一个迭代的过程

- **测试集：** 在所有模型选择和参数调整都完成后，用于**最终评估模型的泛化能力**。它就像是“期末考试”，用来最终检验一个学生学得怎么样


> [!warning] ！！！注意：
> 测试集的数据在模型的整个训练和调参过程中都**从未使用过**，这样才能最真实地反映模型在完全未知数据上的表现



## 前向传播
### 9. 什么是前向传播？
是指数据从神经网络的**输入层**开始，沿着网络结构逐层向前流动，经过每一层神经元的计算（加权求和 + 激活函数），最终在**输出层**产生预测结果的过程
前向前向即是从头到尾往前流的过程

### 10. 什么是激活函数？
- 是神经元中**引入非线性的数学函数**。它的输入是加权求和的结果 z=∑wixi+bz=∑wi​xi​+b，输出是神经元的最终激活值 a=f(z)a=f(z)
- 激活函数主要用于**决定神经元是否“激活”**


### 11. 为什么需要激活函数？
如果所有神经元都没有激活函数（即 a=za=z，线性输出），那么无论多少层神经网络，最终都等价于一个线性变换。而激活函数引入了**非线性**，让神经网络能够拟合任意复杂的函数，就像给团队配备了各种不同性格的员工，能应对各种复杂局面

### 12. 主流的激活函数有什么？
#### 1. Sigmoid —— 温和的“可能派”

**性格**：保守、温和，总是把想法压缩到 0~1 之间

- **公式**：f(z)=11+e−zf(z)=1+e−z1​
    
- **输出范围**：(0, 1)
    

**比喻**：  
你问 Sigmoid 经理：“这个项目能成功吗？”他不会直接说“能”或“不能”，而是说：“我觉得有 80% 的可能性能成功。”他把任何冲动值 zz 都转化为一个概率值：zz 越大，概率越接近 1；zz 越小，越接近 0；z=0z=0 时输出 0.5，表示“不好说”

**适用场景**：
- 多用于二分类问题的**输出层**，输出的是==**概率**==
- 亦可用于**隐藏层**，此时输出的是==**特征强度**==

**缺点**：当 zz 非常大或非常小时，梯度几乎为 0，导致神经元“饱和”，学习停滞（梯度消失）。

---

#### 2. Tanh（双曲正切）—— 情感丰富的“正负派”

**性格**：比 Sigmoid 更丰富，不仅能说“多可能”，还能表达“多讨厌”。

- **公式**：f(z)=ez−e−zez+e−zf(z)=ez+e−zez−e−z​
    
- **输出范围**：(-1, 1)
    

**比喻**：  
你问 Tanh 经理：“这个方案怎么样？”他会说：“我觉得挺不错，给 +0.8 分。”或者“我觉得很糟糕，给 -0.7 分。”他把冲动值映射到 -1 到 1 之间，既有正面也有负面，而且零中心（0 表示中立）。

**适用场景**：隐藏层（早期常用），因为输出有正有负，便于下一层学习。

**缺点**：同样有饱和问题（当 zz 很大或很小时梯度消失）。

---

#### 3. ReLU（修正线性单元）—— 严格的“积极派”

**性格**：非常直接，只传递积极信息，消极信息直接忽略。

- **公式**：f(z)=max⁡(0,z)f(z)=max(0,z)
    
- **输出范围**：[0, +∞)
    

**比喻**：  
ReLU 经理有一个原则：“我只报告好事，坏事我闭嘴。”如果冲动值 zz 是正的（比如 3），他就原样输出 3；如果 zz 是负的（比如 -2），他直接输出 0，啥也不说。

**优点**：计算极快，而且正半轴梯度恒为 1，缓解了梯度消失问题，是目前最常用的隐藏层激活函数。

**缺点**：万一某个神经元总是收到负的冲动值，它就永远输出 0，相当于“死”了（Dead ReLU），再也不会学习。

---

#### 4. Leaky ReLU（泄露修正线性单元）—— 改进的“积极但允许小声嘀咕派”

**性格**：继承了 ReLU 的积极，但允许消极时小声嘀咕一句。

- **公式**：f(z)=max⁡(αz,z)f(z)=max(αz,z)，通常 α=0.01α=0.01
    
- **输出范围**：(-∞, +∞)
    

**比喻**：  
Leaky ReLU 经理说：“好事我大声说，坏事我也小声抱怨一下，但不会完全闭嘴。”当 zz 为负时，他不是输出 0，而是输出一个很小的负数（比如 0.01×zz），这样神经元即使在消极时也能传递一点点信息，避免彻底死亡。

**适用场景**：隐藏层，特别是有 ReLU 死亡风险时。

---

#### 5. Softmax —— 民主选举的“概率分布派”

**性格**：不是单个神经元，而是一组神经元一起工作，把大家的冲动值转化为一个**概率分布**。

- **公式**：
    
- **输出范围**：(0,1)，且所有输出之和为 1
    

**比喻**：  
这是一个选举委员会。有多个候选人（输出层神经元），每个候选人有一个原始得分 zizi​。Softmax 委员会把所有得分转化为**支持率**：谁的原始得分高，支持率就高，而且所有人的支持率加起来正好 100%。

比如图像识别中，10 个类别（0-9）的原始得分是 [2.0, 1.0, 0.1, ...]，经过 Softmax 后变成 [0.7, 0.2, 0.02, ...]，表示“有 70% 的可能是数字 7，20% 是数字 1……”。

**适用场景**：多分类问题的输出层，必须用 Softmax 得到概率。



## 反向传播
### 11. 什么是反向传播？
是训练神经网络的核心算法，它通过**链式法则**高效地计算**损失函数**相对于网络中每个参数（权重和偏置）的梯度，然后利用这些梯度更新参数，从而使模型的预测误差逐步减小

> **==又又打个比方==**
> **团队项目复盘**
> 
> **想象一个团队共同完成一个项目，但最终结果不理想（损失很大）。团队需要找出问题出在哪个环节，并改进**
> 
> **前向传播** = 团队按照分工一步步执行任务，最后提交成果。
> **损失** = 客户反馈说“成果不合格，这里那里有问题”。
> **反向传播** = 团队开始**从后往前**复盘：
>     
>     首先问最后的组装人员（输出层）：“你的组装步骤有没有问题？”（计算输出层梯度）
>         
>     组装人员说：“我这里的零件尺寸不对，应该是上一道工序（最后一个隐藏层）给我的零件有误差。”
>         
>     于是去问上一道工序的负责人：“你给的零件为什么尺寸不对？”（将误差反向传播）
>         
>     那人检查说：“我用的原材料（再前一层的输出）就有问题，是采购部（输入层）买的材料不合格。”
>         
>     … 这样一路追溯到源头，每个环节都清楚自己该负多少责任（梯度）。
> 
> **改进**：每个环节根据自己责任的大小（梯度）调整自己的工作方式（更新参数）。比如采购部下次换供应商，加工部调整加工精度。经过多次这样的复盘迭代，团队最终能交付合格产品

### 12. 神经网络的预测和训练与前向传播和反向传播的关系？
- 训练时，每次都是先前向传播<mark style="background:rgba(240, 107, 5, 0.2)">（得到误差）</mark>，再反向传播<mark style="background:rgba(240, 107, 5, 0.2)">（调整参数）</mark>，反复循环
- 训练完成后，只用前向传播来做预测

> **==打个比方==**
> **训练阶段 = 学习弹钢琴**
> 
> - **前向传播** = 你看着乐谱弹一遍曲子（得到弹奏结果）
>     
> - **计算损失** = 老师指出你哪里弹错了（误差）
>     
> - **反向传播** = 老师告诉你每个手指该怎么调整（梯度）
>     
> - **更新参数** = 你根据建议调整手指姿势（更新权重）
> 
> 然后**再弹一遍**（再次前向传播）→ 老师再点评（计算损失）→ 再调整（反向传播）→ 如此反复，直到弹得越来越好


